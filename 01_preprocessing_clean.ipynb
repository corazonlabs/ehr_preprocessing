{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp preprocessing.clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing.clean\n",
    "\n",
    "> Functions to split the raw FHIR dataset, clean and save for further processing & vocab creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "[Data Dictionary - Listing Condition Codes](https://docs.google.com/spreadsheets/d/17lR6rpCnSg2B5sLYQjz6yLkwPrK3UgeSn8iE83tZ54Y/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Health and Economic Costs of Chronic Diseases - CDC](https://www.cdc.gov/chronicdisease/about/costs/index.htm)\n",
    "\n",
    "### Synthea\n",
    "- [Synthea - Wiki](https://github.com/synthetichealth/synthea/wiki)\n",
    "    - contains details about what the project is and how to get started and generate the data.\n",
    "- [Synthea Data Dictionary](https://github.com/synthetichealth/synthea/wiki/CSV-File-Data-Dictionary)\n",
    "\n",
    "#### Data Generation\n",
    "- For our purposes, once Synthea is set up, the following scripts generate the data. \n",
    "- Its important to record the run dates, we will use this during preprocessing.\n",
    "\n",
    "`./run_synthea -s 12345 -p 10000`\n",
    "- run date: 12-19-2019\n",
    "- {alive=10000, dead=1076}\n",
    "- raw - 1.5GB\n",
    "\n",
    "`./run_synthea -s 54321 -p 20000`\n",
    "- run date: 11-5-2020\n",
    "- {alive=20000, dead=2195}\n",
    "- csv - 2.9GB\n",
    "\n",
    "`./run_synthea -s 12345 -p 100000`\n",
    "- run date: 4-4-2020\n",
    "- {alive=100000, dead=10872}\n",
    "- csv - 14.6GB\n",
    "\n",
    "### Folder Structure\n",
    "- Choose a location to store all Synthea-generated data (\n",
    "    - for example `~/mydatastore`\n",
    "- Copy data generated by Synthea into this specific folder structure \n",
    "    - for 10K data - `mydatastore/datasets/synthea/10K/raw_original`\n",
    "- Then create a symbolic link called `datasets` in the current working folder pointing to the datasets folder in the datastore \n",
    "    - `ln -sfn ~/mydatastore/datasets datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.imports import *\n",
    "from fastai import *\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert torch.cuda.is_available() == True\n",
    "assert torch.backends.cudnn.enabled == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x7f3e2aba2910>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "# cuda0 = torch.device('cuda:0')\n",
    "device = torch.device('cuda',0)\n",
    "torch.cuda.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "PATH = \".\"\n",
    "PATH_1K = f'{PATH}/datasets/synthea/1K'\n",
    "PATH_10K = f'{PATH}/datasets/synthea/10K'\n",
    "PATH_20K = f'{PATH}/datasets/synthea/20K'\n",
    "PATH_100K = f'{PATH}/datasets/synthea/100K'\n",
    "FILENAMES = ['patients', 'observations', 'allergies', 'careplans', 'medications', 'imaging_studies', 'procedures', 'conditions', 'immunizations']\n",
    "SYNTHEA_DATAGEN_DATES = dict({'1K':'11-16-2018', '10K':'12-19-2019', '20K':'11-5-2020', '100K':'4-4-2020', '250K':'11-16-2018'})\n",
    "CONDITIONS = dict({'diabetes':'44054006||START', 'stroke':'230690007||START', 'alzheimers':'26929004||START', 'coronary_heart':'53741008||START'})\n",
    "LABELS = ['diabetes', 'stroke', 'alzheimers', 'coronaryheart']\n",
    "CUTOFF_AGE = 20\n",
    "LOG_NUMERICALIZE_EXCEP = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['patients.csv',\n",
       " 'observations.csv',\n",
       " 'allergies.csv',\n",
       " 'careplans.csv',\n",
       " 'medications.csv',\n",
       " 'imaging_studies.csv',\n",
       " 'procedures.csv',\n",
       " 'conditions.csv',\n",
       " 'encounters.csv',\n",
       " 'immunizations.csv']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(f'{PATH_1K}/raw_original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_raw_ehrdata(path, csv_names = FILENAMES):\n",
    "    '''Read raw EHR data'''\n",
    "    dfs = [pd.read_csv(f'{path}/{fname}.csv', low_memory=False) for fname in csv_names]\n",
    "    return dfs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = read_raw_ehrdata(f'{PATH_1K}/raw_original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients, observations, allergies, careplans, medications, imaging_studies, procedures, conditions, immunizations = dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def split_patients(patients, valid_pct=0.2, test_pct=0.2, random_state=1234):\n",
    "    '''Split the patients dataframe'''\n",
    "    train_pct = 1 - (valid_pct + test_pct)\n",
    "    print(f'Splits:: train: {train_pct}, valid: {valid_pct}, test: {test_pct}')\n",
    "    patients = patients.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    return np.split(patients, [int(train_pct*len(patients)), int((train_pct+valid_pct)*len(patients))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:: train: 0.7, valid: 0.2, test: 0.1\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = split_patients(patients, .2,.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1108, 775, 222, 111)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(patients), len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(patients) == len(train)+len(valid)+len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def split_ehr_dataset(path, valid_pct=0.2, test_pct=0.2, random_state=1234):\n",
    "    '''Split EHR dataset into train, valid, test and save'''\n",
    "\n",
    "    train_dfs, valid_dfs, test_dfs = [],[],[]\n",
    "    \n",
    "    dfs = read_raw_ehrdata(f'{path}/raw_original')\n",
    "    train_pt, valid_pt, test_pt = split_patients(dfs[0], valid_pct, test_pct, random_state)\n",
    "    train_dfs.append(train_pt)\n",
    "    valid_dfs.append(valid_pt)\n",
    "    test_dfs.append(test_pt)\n",
    "    print(f'Split {FILENAMES[0]} into:: Train: {len(train_pt)}, Valid: {len(valid_pt)}, Test: {len(test_pt)} -- Total before split: {len(dfs[0])}')\n",
    "    \n",
    "    for df, name in zip(dfs[1:], FILENAMES[1:]):\n",
    "        df = df.set_index('PATIENT')\n",
    "        df_train = df.loc[df.index.intersection(train_pt['ID']).unique()]\n",
    "        df_valid = df.loc[df.index.intersection(valid_pt['ID']).unique()]\n",
    "        df_test = df.loc[df.index.intersection(test_pt['ID']).unique()]\n",
    "        assert len(df) == len(df_train)+len(df_valid)+len(df_test),f'Split failed {name}: {len(df)} != {len(df_train)}+{len(df_valid)}+{len(df_test)}'\n",
    "        train_dfs.append(df_train.reset_index())\n",
    "        valid_dfs.append(df_valid.reset_index())\n",
    "        test_dfs.append(df_test.reset_index())\n",
    "\n",
    "    \n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        d = Path(f'{path}/raw_split/{split}')\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if split == 'train':\n",
    "            for df, name in zip(train_dfs, FILENAMES):\n",
    "                df.to_csv(f'{d}/{name}.csv', index=False)\n",
    "            print(f'Saved train data to {d}')\n",
    "        \n",
    "        if split == 'valid':\n",
    "            for df, name in zip(valid_dfs, FILENAMES):\n",
    "                df.to_csv(f'{d}/{name}.csv', index=False)\n",
    "            print(f'Saved valid data to {d}')\n",
    "    \n",
    "        if split == 'test':\n",
    "            for df, name in zip(test_dfs, FILENAMES):\n",
    "                df.to_csv(f'{d}/{name}.csv', index=False)\n",
    "            print(f'Saved test data to {d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:: train: 0.6, valid: 0.2, test: 0.2\n",
      "Split patients into:: Train: 664, Valid: 222, Test: 222 -- Total before split: 1108\n",
      "Saved train data to datasets/synthea/1K/raw_split/train\n",
      "Saved valid data to datasets/synthea/1K/raw_split/valid\n",
      "Saved test data to datasets/synthea/1K/raw_split/test\n"
     ]
    }
   ],
   "source": [
    "split_ehr_dataset(PATH_1K) #will use default values for split percents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cleanup_pts(pts, is_train, today=None):\n",
    "    '''Clean patients df'''\n",
    "    pts.drop(columns=['SSN','DRIVERS','PASSPORT','PREFIX','FIRST','LAST','SUFFIX','MAIDEN','ADDRESS'], inplace=True)\n",
    "    pts.rename(str.lower, axis='columns', inplace=True)\n",
    "    pts.rename(columns={\"id\":\"patient\"}, inplace=True)\n",
    "    pts = pts.astype({'birthdate':'datetime64'}) \n",
    "    pts['zip'] = pts['zip'].fillna(0.0).astype(int)    \n",
    "    if today == None: today = pd.Timestamp.today()\n",
    "    else            : today = pd.to_datetime(today)\n",
    "    pts['age_now_days'] = pts['birthdate'].apply(lambda bday: (today-bday).days)\n",
    "    \n",
    "    pts.set_index('patient', inplace=True)\n",
    "    pt_demographics = pts.loc[:,['birthdate', 'marital', 'race', 'ethnicity', 'gender', 'birthplace', 'city', 'state', 'zip', 'age_now_days']] \n",
    "    patients = pts.loc[:,['birthdate']]\n",
    "    if is_train: pt_codes = pt_demographics.reset_index(drop=True)\n",
    "    \n",
    "    return [patients, pt_demographics, pt_codes] if is_train else [patients, pt_demographics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cleanup_obs(obs, is_train):\n",
    "    '''Clean observations df'''\n",
    "    obs.UNITS.fillna('xxxnan', inplace=True)\n",
    "    obs.dropna(subset=['VALUE'], inplace=True)\n",
    "    obs.rename(columns={\"DATE\":\"date\", \"PATIENT\":\"patient\", \"CODE\":\"orig_code\", \\\n",
    "                                 \"DESCRIPTION\":\"desc\", \"VALUE\":\"value\", \"UNITS\":\"units\", \"TYPE\":\"type\"}, inplace=True)\n",
    "    obs['code'] = obs['orig_code'].str.cat(obs[['value', 'units', 'type']].astype(str), sep='||')\n",
    "\n",
    "    if is_train: obs_codes = obs.loc[:, ['orig_code', 'desc', 'value', 'units', 'type']]\n",
    "    \n",
    "    obs = obs.loc[:, ['patient', 'date', 'code']]\n",
    "    obs = obs.astype({'date':'datetime64'})\n",
    "    obs.set_index('patient', inplace=True)\n",
    "    \n",
    "    return [obs, obs_codes] if is_train else [obs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Drops rows with null in the `VALUE` column\n",
    "- Creates a new `code` column with a concatenation of `code`, `value`, `units` and `type`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vocab creation for Observations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For `numeric`**\n",
    "```\n",
    "for 'numeric'\n",
    "    get unique 'codes'\n",
    "    for each unique code\n",
    "        get unique 'units'\n",
    "            for each unique unit\n",
    "                bucketize 'values'\n",
    "                create vocab entry for each 'bucket' -- code||value_bucket||units\n",
    "```\n",
    "**For `text`**\n",
    "```\n",
    "for 'text'\n",
    "    get unique 'codes'\n",
    "    for each unique code\n",
    "        get unique 'units' #this will be null\n",
    "            for each unique unit\n",
    "                get unique 'values'\n",
    "                create vocab entry for each -- code||value||units\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cleanup_algs(allergies, is_train):\n",
    "    '''Clean allergies df'''\n",
    "    allergies.drop(columns=['ENCOUNTER'], inplace=True)\n",
    "    \n",
    "    stops = pd.DataFrame(allergies.loc[allergies['STOP'].notnull(),:])\n",
    "    allergies['CODE'] = allergies['CODE'].apply(lambda x: f'{str(x)}||START')\n",
    "    stops['CODE'] = stops['CODE'].apply(lambda x: f'{str(x)}||STOP')\n",
    "    allergies.drop(columns=['STOP'], inplace=True)\n",
    "    stops.drop(columns=['START'], inplace=True)\n",
    "    allergies.rename(columns={\"START\":\"date\", \"PATIENT\":\"patient\", \"CODE\":\"code\", \"DESCRIPTION\":\"desc\"}, inplace=True)\n",
    "    stops.rename(columns={\"STOP\":\"date\", \"PATIENT\":\"patient\", \"CODE\":\"code\", \"DESCRIPTION\":\"desc\"}, inplace=True)\n",
    "    allergies = allergies.append(stops, ignore_index=True)\n",
    "    \n",
    "    if is_train: alg_codes = allergies.loc[:, ['code', 'desc']]\n",
    "        \n",
    "    allergies.drop(columns=['desc'], inplace=True)\n",
    "    allergies = allergies.astype({'date':'datetime64'})\n",
    "    allergies.set_index('patient', inplace=True)\n",
    "    return [allergies, alg_codes] if is_train else [allergies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cleanup_crpls(careplans, is_train):\n",
    "    '''Clean careplans df'''\n",
    "    careplans.drop(columns=['ID', 'ENCOUNTER', 'REASONCODE', 'REASONDESCRIPTION'], inplace=True)\n",
    "    \n",
    "    stops = pd.DataFrame(careplans.loc[careplans['STOP'].notnull(),:])\n",
    "    careplans['CODE'] = careplans['CODE'].apply(lambda x: f'{str(x)}||START')\n",
    "    stops['CODE'] = stops['CODE'].apply(lambda x: f'{str(x)}||STOP')\n",
    "    careplans.drop(columns=['STOP'], inplace=True)\n",
    "    stops.drop(columns=['START'], inplace=True)\n",
    "    careplans.rename(columns={\"START\":\"date\", \"PATIENT\":\"patient\", \"CODE\":\"code\", \"DESCRIPTION\":\"desc\"}, inplace=True)\n",
    "    stops.rename(columns={\"STOP\":\"date\", \"PATIENT\":\"patient\", \"CODE\":\"code\", \"DESCRIPTION\":\"desc\"}, inplace=True)\n",
    "    careplans = careplans.append(stops, ignore_index=True)\n",
    "    \n",
    "    if is_train: crpl_codes = careplans.loc[:, ['code', 'desc']]\n",
    "\n",
    "    careplans.drop(columns=['desc'], inplace=True)\n",
    "    careplans = careplans.astype({'date':'datetime64'})\n",
    "    careplans.set_index('patient', inplace=True)\n",
    "    return [careplans, crpl_codes] if is_train else [careplans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cleanup_meds(medications, is_train):\n",
    "    '''Clean `medications` df'''\n",
    "    medications.drop(columns=['ENCOUNTER', 'COST', 'DISPENSES', 'TOTALCOST', 'REASONCODE', 'REASONDESCRIPTION'], inplace=True)\n",
    "    stops = pd.DataFrame(medications.loc[medications['STOP'].notnull(),:])\n",
    "    medications['CODE'] = medications['CODE'].apply(lambda x: f'{str(x)}||START')\n",
    "    stops['CODE'] = stops['CODE'].apply(lambda x: f'{str(x)}||STOP')\n",
    "    medications.drop(columns=['STOP'], inplace=True)\n",
    "    stops.drop(columns=['START'], inplace=True)\n",
    "    medications.rename(columns={\"START\":\"date\", \"PATIENT\":\"patient\", \"CODE\":\"code\", \"DESCRIPTION\":\"desc\"}, inplace=True)\n",
    "    stops.rename(columns={\"STOP\":\"date\", \"PATIENT\":\"patient\", \"CODE\":\"code\", \"DESCRIPTION\":\"desc\"}, inplace=True)\n",
    "    medications = medications.append(stops, ignore_index=True)\n",
    "    \n",
    "    if is_train: med_codes = medications.loc[:, ['code', 'desc']]\n",
    "\n",
    "    medications.drop(columns=['desc'], inplace=True)\n",
    "    medications = medications.astype({'date':'datetime64'})\n",
    "    medications.set_index('patient', inplace=True)\n",
    "    return [medications, med_codes] if is_train else [medications]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cleanup_img(imaging_studies, is_train):\n",
    "    '''Clean `imaging` df'''\n",
    "    imaging_studies.rename(columns={\"DATE\":\"date\", \"PATIENT\":\"patient\", \"BODYSITE_CODE\":\"code\", \"BODYSITE_DESCRIPTION\":\"desc\"}, inplace=True)\n",
    "\n",
    "    if is_train: img_codes = imaging_studies.loc[:, ['code', 'desc']]\n",
    "        \n",
    "    imaging_studies = imaging_studies.loc[:, ['patient', 'date', 'code']]\n",
    "    imaging_studies = imaging_studies.astype({'date':'datetime64'})\n",
    "    imaging_studies.set_index('patient', inplace=True)\n",
    "    return [imaging_studies, img_codes] if is_train else [imaging_studies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cleanup_procs(procedures, is_train):\n",
    "    '''Clean `procedures` df'''\n",
    "    procedures.rename(columns={\"DATE\":\"date\", \"PATIENT\":\"patient\", \"CODE\":\"code\", \"DESCRIPTION\":\"desc\"}, inplace=True)\n",
    "\n",
    "    if is_train: proc_codes = procedures.loc[:, ['code', 'desc']]\n",
    "    \n",
    "    procedures = procedures.loc[:, ['patient', 'date', 'code']]\n",
    "    procedures = procedures.astype({'date':'datetime64'})\n",
    "    procedures.set_index('patient', inplace=True)\n",
    "    return [procedures, proc_codes] if is_train else [procedures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cleanup_cnds(conditions, is_train):\n",
    "    '''Clean `conditions` df'''\n",
    "    conditions.drop(columns=['ENCOUNTER'], inplace=True)\n",
    "    stops = pd.DataFrame(conditions.loc[conditions['STOP'].notnull(),:])\n",
    "    conditions['CODE'] = conditions['CODE'].apply(lambda x: f'{str(x)}||START')\n",
    "    stops['CODE'] = stops['CODE'].apply(lambda x: f'{str(x)}||STOP')\n",
    "    conditions.drop(columns=['STOP'], inplace=True)\n",
    "    stops.drop(columns=['START'], inplace=True)\n",
    "    conditions.rename(columns={\"START\":\"date\", \"PATIENT\":\"patient\", \"CODE\":\"code\", \"DESCRIPTION\":\"desc\"}, inplace=True)\n",
    "    stops.rename(columns={\"STOP\":\"date\", \"PATIENT\":\"patient\", \"CODE\":\"code\", \"DESCRIPTION\":\"desc\"}, inplace=True)\n",
    "    conditions = conditions.append(stops, ignore_index=True)\n",
    "        \n",
    "    if is_train: cnd_codes = conditions.loc[:, ['code', 'desc']]\n",
    "        \n",
    "    conditions.drop(columns=['desc'], inplace=True)\n",
    "    conditions = conditions.astype({'date':'datetime64'})\n",
    "    conditions.set_index('patient', inplace=True)\n",
    "    return [conditions, cnd_codes] if is_train else [conditions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cleanup_immns(immunizations, is_train):\n",
    "    immunizations.rename(columns={\"DATE\":\"date\", \"PATIENT\":\"patient\", \"CODE\":\"code\", \"DESCRIPTION\":\"desc\"}, inplace=True)\n",
    "\n",
    "    if is_train: imm_codes = immunizations.loc[:, ['code', 'desc']]\n",
    "        \n",
    "    immunizations = immunizations.loc[:, ['patient', 'date', 'code']]\n",
    "    immunizations = immunizations.astype({'date':'datetime64'})\n",
    "    immunizations.set_index('patient', inplace=True)\n",
    "    return [immunizations, imm_codes] if is_train else [immunizations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cleanup_dataset(path, is_train, today=None):\n",
    "    '''Clean all dfs in a split'''\n",
    "    dfs = read_raw_ehrdata(path)\n",
    " \n",
    "    pt_data   = cleanup_pts(dfs[0],   is_train, today)\n",
    "    obs_data  = cleanup_obs(dfs[1],   is_train)\n",
    "    alg_data  = cleanup_algs(dfs[2],  is_train)\n",
    "    crpl_data = cleanup_crpls(dfs[3], is_train)\n",
    "    med_data  = cleanup_meds(dfs[4],  is_train)\n",
    "    img_data  = cleanup_img(dfs[5],   is_train)\n",
    "    proc_data = cleanup_procs(dfs[6], is_train)\n",
    "    cnd_data  = cleanup_cnds(dfs[7],  is_train)\n",
    "    imm_data  = cleanup_immns(dfs[8], is_train)   \n",
    "    \n",
    "    data_tables = [pt_data[0], pt_data[1], obs_data[0], alg_data[0], crpl_data[0], med_data[0], img_data[0], proc_data[0], cnd_data[0], imm_data[0]]\n",
    "    if is_train:\n",
    "        code_tables = [pt_data[2], obs_data[1], alg_data[1], crpl_data[1], med_data[1], img_data[1], proc_data[1], cnd_data[1], imm_data[1]]\n",
    "    \n",
    "    return (data_tables, code_tables) if is_train else (data_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tables, code_tables = cleanup_dataset(f'{PATH_1K}/raw_split/train', is_train=True)\n",
    "\n",
    "patients, pt_demographics, observations, allergies, \\\n",
    "careplans, medications, imaging_studies, procedures, conditions, immunizations = data_tables\n",
    "\n",
    "pt_codes, obs_codes, alg_codes, crpl_codes, med_codes, img_codes, proc_codes, cnd_codes, imm_codes = code_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date    26426\n",
       "code    26426\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orig_code    370128\n",
       "desc         370128\n",
       "value        370128\n",
       "units        370128\n",
       "type         370128\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_codes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Labels (y) & Insert Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels we intend to predict are conditions and must be in the `CONDITIONS` dict\n",
    "- Adding them to the `patients` df\n",
    "- And adding the patient's age when the condition was recorded\n",
    "\n",
    "Also ..\n",
    "- Inserting patient's age in months and years into each record df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetes :: 44054006||START\n",
      "stroke :: 230690007||START\n",
      "alzheimers :: 26929004||START\n",
      "coronary_heart :: 53741008||START\n"
     ]
    }
   ],
   "source": [
    "for key in CONDITIONS.keys():\n",
    "    print(key,\"::\",CONDITIONS[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_ys(patients, conditions, cnd_dict=CONDITIONS):\n",
    "    '''Extract labels from conditions df and add them to patients df with age'''\n",
    "    for key in cnd_dict.keys():\n",
    "        patients = patients.merge(conditions[conditions.code==cnd_dict[key]], how='left', left_index=True, right_index=True)\n",
    "        patients[f'{key}_y'] = patients.code.notna()\n",
    "        patients[f'{key}_age'] = ((patients.date - patients.birthdate)//np.timedelta64(1,'Y'))\n",
    "        patients = patients.drop(columns=['date','code'])\n",
    "    return patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_pts = extract_ys(patients, conditions, CONDITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "birthdate             664\n",
       "diabetes_y            664\n",
       "diabetes_age           37\n",
       "stroke_y              664\n",
       "stroke_age             42\n",
       "alzheimers_y          664\n",
       "alzheimers_age         18\n",
       "coronary_heart_y      664\n",
       "coronary_heart_age     34\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_pts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def insert_age(df, pts_df):\n",
    "    '''Insert age in years and months into each of the rec dfs'''\n",
    "    df = df.merge(pts_df, left_index=True, right_index=True)\n",
    "    df['age']        = (df['date'] - df['birthdate'])//np.timedelta64(1,'Y')\n",
    "    df['age_months'] = (df['date'] - df['birthdate'])//np.timedelta64(1,'M')\n",
    "    return df.drop(columns=['date','birthdate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do All Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def clean_raw_ehrdata(path, valid_pct=0.2, test_pct=0.2, today=None):\n",
    "    '''Split, clean, preprocess & save raw EHR data'''\n",
    "    split_ehr_dataset(path, valid_pct, test_pct)\n",
    "    \n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        split_path = f'{path}/raw_split/{split}'\n",
    "        if split == 'train': data_tables, code_tables = cleanup_dataset(split_path, is_train=True, today=today)\n",
    "        else               : data_tables = cleanup_dataset(split_path, is_train=False)\n",
    "        patients, conditions, rec_tables = data_tables[0], data_tables[8], data_tables[2:]\n",
    "        patients = extract_ys(patients, conditions, CONDITIONS)\n",
    "        rec_dfs = [insert_age(rec_df, pd.DataFrame(patients.birthdate)) for rec_df in rec_tables]\n",
    "        \n",
    "        cleaned_dir = Path(f'{path}/cleaned/{split}')\n",
    "        cleaned_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "        for rec_df,name in zip(rec_dfs,FILENAMES[1:]):\n",
    "            rec_df.to_csv(f'{cleaned_dir}/{name}.csv')\n",
    "        patients.reset_index(inplace=True)\n",
    "        patients.to_csv(f'{cleaned_dir}/patients.csv', index_label='indx')\n",
    "        data_tables[1].to_csv(f'{cleaned_dir}/patient_demographics.csv')\n",
    "        print(f'Saved cleaned \"{split}\" data to {cleaned_dir}')\n",
    "        \n",
    "        if split == 'train':\n",
    "            codes_dir = Path(f'{cleaned_dir}/codes')\n",
    "            codes_dir.mkdir(parents=True, exist_ok=True)\n",
    "            for code_df,name in zip(code_tables, FILENAMES):\n",
    "                code_df.to_csv(f'{codes_dir}/code_{name}.csv', index_label='indx')\n",
    "            print(f'Saved vocab code tables to {codes_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_cleaned_ehrdata(path):\n",
    "    '''Load cleaned, age-filtered EHR data'''\n",
    "    \n",
    "    csv_names = FILENAMES.copy()\n",
    "    csv_names.insert(1,'patient_demographics')\n",
    "    \n",
    "    train_dfs = [pd.read_csv(f'{path}/cleaned/train/{fname}.csv', low_memory=False, index_col=0) for fname in csv_names]\n",
    "    valid_dfs = [pd.read_csv(f'{path}/cleaned/valid/{fname}.csv', low_memory=False, index_col=0) for fname in csv_names]\n",
    "    test_dfs  = [pd.read_csv(f'{path}/cleaned/test/{fname}.csv', low_memory=False, index_col=0) for fname in csv_names]\n",
    "                             \n",
    "    return train_dfs, valid_dfs, test_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_ehr_vocabcodes(path):\n",
    "    '''Load codes for vocabs'''\n",
    "    \n",
    "    code_dfs = [pd.read_csv(f'{path}/cleaned/train/codes/code_{fname}.csv', low_memory=False, na_filter=False, index_col=0) for fname in FILENAMES]\n",
    "                             \n",
    "    return code_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "train_dfs, valid_dfs, test_dfs = load_cleaned_ehrdata(PATH_1K)\n",
    "code_dfs = load_ehr_vocabcodes(PATH_1K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# for df in train_dfs:\n",
    "#     display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# for df in code_dfs:\n",
    "#     display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_preprocessing_clean.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
